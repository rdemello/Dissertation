{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_rows = 4000\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import arange\n",
    "from numpy import interp\n",
    "import math\n",
    "import os\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import dump, load\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "df = pd.read_csv('02_Data/WDIData.csv')  \n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "\n",
    "countrycode = pd.read_csv('02_Data/COUNTRYCODES.csv')  \n",
    "countrycode.columns = [c.replace(' ', '_') for c in countrycode.columns]\n",
    "\n",
    "predictions = pd.read_csv('02_Data/PREDICTIONS.csv')  \n",
    "predictions.columns = [c.replace(' ', '_') for c in predictions.columns]\n",
    "\n",
    "SSP_Data = pd.read_csv('02_Data/SSP.csv')  \n",
    "SSP_Data.columns = [c.replace(' ', '_') for c in SSP_Data.columns]\n",
    "\n",
    "WDI_Data = pd.read_csv('02_Data/WDIData.csv')  \n",
    "WDI_Data.columns = [c.replace(' ', '_') for c in WDI_Data.columns]\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"ENR\", \"FDI\", \"GDP\", \"IND\", \"RND\", \"POP\", \"TRD\", \"URB\", \"CO2\"]\n",
    "epochs = 1000\n",
    "batchs = 32\n",
    "testCountries = [\"United Kingdom\"]\n",
    "top30Countries = [\"China\", \"United States\",\"India\", \"Russian Federation\", \"Japan\", \n",
    "                  \"Canada\", \"Germany\", \"Korea, Rep.\", \"Brazil\", \"France\", \"Saudi Arabia\", \n",
    "                  \"United Kingdom\", \"Pakistan\", \"Mexico\", \"Iran, Islamic Rep.\", \"Turkey\", \"Italy\", \"Spain\", \n",
    "                  \"Indonesia\", \"Australia\", \"South Africa\", \"Vietnam\", \"Egypt, Arab Rep.\", \n",
    "                  \"Thailand\", \"Argentina\",\"Nigeria\",\"Poland\", \"Malaysia\", \"Venezuela, RB\", \"Congo, Dem. Rep.\"]\n",
    "secondbatch = [\"Spain\", \n",
    "                  \"Indonesia\", \"Australia\", \"South Africa\", \"Vietnam\", \"Egypt, Arab Rep.\", \n",
    "                  \"Thailand\", \"Argentina\",\"Nigeria\",\"Poland\", \"Malaysia\", \"Venezuela, RB\", \"Congo, Dem. Rep.\"]\n",
    "countryAdditions = [\"Venezuela, RB\", \"Congo, Dem. Rep.\",\"Nigeria\", \"Pakistan\", \"Argentina\"]\n",
    "updatedCountries = [\"Brazil\", \"China\"]\n",
    "top6Countries = [\"China\", \"United States\", \"India\", \"Russian Federation\", \"Japan\", \"United Kingdom\"]\n",
    "\n",
    "def getRawData(country):\n",
    "    country_data = df.loc[df['Country_Name'] == country]\n",
    "    country_data = country_data.fillna(0)\n",
    "    country_data = country_data.drop(['Country_Code','Country_Name','Unnamed:_64','Indicator_Code'], 1)\n",
    "    #Total greenhouse gas emissions (kt of CO2 equivalent)\n",
    "    #Fossil fuel energy consumption (% of total)\n",
    "    ENR = country_data.loc[country_data['Indicator_Name'] == \"Energy use (kg of oil equivalent per capita)\"] \n",
    "    FDI = country_data.loc[country_data['Indicator_Name'] == \"Foreign direct investment, net inflows (% of GDP)\"] \n",
    "    GDP = country_data.loc[country_data['Indicator_Name'] == \"GDP per capita (current US$)\"] \n",
    "    IND = country_data.loc[country_data['Indicator_Name'] == \"Industry (including construction), value added (% of GDP)\"] \n",
    "    RND = country_data.loc[country_data['Indicator_Name'] == \"Trademark applications, total\"] \n",
    "    POP = country_data.loc[country_data['Indicator_Name'] == \"Population, total\"] \n",
    "    TRD = country_data.loc[country_data['Indicator_Name'] == \"Trade (% of GDP)\"] \n",
    "    URB = country_data.loc[country_data['Indicator_Name'] == \"Urban population (% of total population)\"]\n",
    "    CO2 = country_data.loc[country_data['Indicator_Name'] == \"CO2 intensity (kg per kg of oil equivalent energy use)\"]\n",
    "\n",
    "    ALL = [ENR, FDI, GDP, IND, RND, POP, TRD, URB, CO2]\n",
    "    INPUT = pd.concat(ALL)\n",
    "    INPUT = INPUT.transpose()\n",
    "    INPUT.columns = titles\n",
    "    INPUT = INPUT.iloc[1:]\n",
    "    return INPUT\n",
    "\n",
    "def getAllData(country, modelVersion, noZeroes, dumpScaler):\n",
    "    #get the initial dataset\n",
    "    df = getRawData(country)\n",
    "    df = df[20:]\n",
    "    df = df.head(35)\n",
    "    \n",
    "    if noZeroes:\n",
    "        df = df.astype(float)\n",
    "        df.index = df.index.map(int)\n",
    "\n",
    "        for column in df:\n",
    "            if(df[column] == 0).any():\n",
    "                df[column].replace(0, np.nan, inplace=True)\n",
    "                df[column] = df[column].interpolate(method=\"linear\", limit_direction=\"both\", limit_area=\"inside\")\n",
    "                df[column]  = df[column].fillna(0)\n",
    "    \n",
    "    #start interpolation\n",
    "    df = df.reset_index()\n",
    "    years = df[\"index\"].to_numpy().reshape(-1).astype(np.int16)\n",
    "    adjustedYears = np.zeros(len(years)).astype(np.float32)\n",
    "\n",
    "    #adjust years for where annual number should end up e.g. 0.5 = midpoint of year\n",
    "    for i, x in enumerate(years):\n",
    "        adjustedYears[i] = x+0.5\n",
    "\n",
    "    #get array for interpolated years with quarterly steps\n",
    "    newYears = np.arange(start=1980, stop=2015, step=0.25)\n",
    "    newYears = newYears.astype(np.float32)\n",
    "    newNP = np.zeros((141,10))\n",
    "\n",
    "    #interpolation for the annual average indices\n",
    "    for i, x in enumerate(titles):\n",
    "        indicator = df[[str(x)]].to_numpy().reshape(-1)\n",
    "        thisInterpolation = interpolate.interp1d(adjustedYears, indicator, kind=\"quadratic\", fill_value='extrapolate')\n",
    "        for j, y in enumerate(newYears):\n",
    "            newNP[j+1,i+1] = thisInterpolation(y)\n",
    "\n",
    "    #create DF\n",
    "    newDF = pd.DataFrame(data=newNP[1:,1:])\n",
    "    newDF[\"Year\"] = newYears\n",
    "    newDF = newDF.set_index(\"Year\")\n",
    "    newDF.columns = titles\n",
    "\n",
    "    train_df = newDF.sample(frac=0.85,random_state=0)\n",
    "    test_df = newDF.drop(train_df.index)\n",
    "\n",
    "    input_df = train_df.drop('CO2', axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    input_NP = input_df.to_numpy()\n",
    "    input_NP = scaler.fit_transform(input_NP)\n",
    "    if dumpScaler:\n",
    "        dump(scaler, 'Countries/{}/Models/{}/Input_Scaler.bin'.format(country, modelVersion), compress=True)\n",
    "\n",
    "    output_df = train_df.drop([\"ENR\", \"FDI\", \"GDP\", \"IND\", \"RND\", \"POP\", \"TRD\", \"URB\"], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    output_NP = output_df.to_numpy()\n",
    "    output_NP = scaler.fit_transform(output_NP)\n",
    "    if dumpScaler:\n",
    "        dump(scaler, 'Countries/{}/Models/{}/Output_Scaler.bin'.format(country, modelVersion), compress=True)\n",
    "    \n",
    "    return df, newNP, newDF, train_df, test_df, input_NP, input_df, output_NP, output_df\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(5, activation=\"relu\", input_dim=8),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimiser = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                 optimizer=optimiser,\n",
    "                 metrics=['mae','mse'])\n",
    "    return model\n",
    "\n",
    "def checkTest(country, testData, model, modelVersion):\n",
    "    if 'CO2' in testData.columns:\n",
    "        test_labels = testData.pop('CO2')\n",
    "    scaler = load('Countries/{}/Models/{}/Input_Scaler.bin'.format(country, modelVersion))\n",
    "    normed_test_data  = scaler.transform(testData)\n",
    "\n",
    "    test_predictions = model.predict(normed_test_data).flatten()\n",
    "    outputScaler = load('Countries/{}/Models/{}/Output_Scaler.bin'.format(country, modelVersion))\n",
    "    finalOutput = outputScaler.inverse_transform(test_predictions)\n",
    "\n",
    "    error = finalOutput - test_labels\n",
    "    \n",
    "    return test_labels, finalOutput, error, normed_test_data\n",
    "\n",
    "def createANN(chosenCountry, printTraining, printR, snsPlot, modelVersion, txtLog):\n",
    "    if not os.path.exists('Countries/{}/Models/{}'.format(chosenCountry, modelVersion)):\n",
    "        os.makedirs('Countries/{}/Models/{}'.format(chosenCountry, modelVersion))\n",
    "    \n",
    "    allData, all_NP, all_df, trainingDataFull, testData, input_NP, input_df, output_NP, output_df = getAllData(chosenCountry, modelVersion, True)\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model=build_model()\n",
    "    \n",
    "    history = model.fit(\n",
    "        input_NP, \n",
    "        output_NP,\n",
    "        epochs=epochs,\n",
    "        batch_size = batchs,\n",
    "        validation_split = 0.15,\n",
    "        verbose = 0,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    epochsRun = len(history.history['loss'])\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    hist.to_csv('Countries/{}/Models/{}/history.csv'.format(chosenCountry, modelVersion))\n",
    "    \n",
    "    \n",
    "    actualVal, predictedVal, errorVal, normed_test_data = checkTest(chosenCountry, testData, model, modelVersion)\n",
    "    r2Val = math.ceil(r2_score(actualVal, predictedVal)*100)/100\n",
    "    \n",
    "    if printTraining:\n",
    "        printTrainingGraph(history, chosenCountry, modelVersion)\n",
    "\n",
    "    if printR:\n",
    "        printRGraph(actualVal, predictedVal, errorVal, r2Val, modelVersion, chosenCountry)\n",
    "\n",
    "    if snsPlot:\n",
    "        sns.pairplot(input_df)\n",
    "    \n",
    "    stamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    \n",
    "    if r2Val < 0:\n",
    "        txtLog.write(stamp + \" :: NEGATIVE R2: \"+ str(r2Val)+\" in \"+str(chosenCountry)+\" Model \"+ str(modelVersion) +\" \\n\")\n",
    "        r2values = pd.DataFrame({'Actual': actualVal, 'Predicted': predictedVal}, columns=['Actual', 'Predicted'])\n",
    "        r2values.to_csv('Countries/{}/Models/{}/r2Values.csv'.format(chosenCountry, modelVersion))\n",
    "        \n",
    "    return model, r2Val, history, input_df, epochsRun\n",
    "\n",
    "def printTrainingGraph(history, chosenCountry, modelVersion):\n",
    "    plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "    plt.figure(1)\n",
    "    plotter.plot({'Basic':history}, metric=\"mse\")\n",
    "    plt.ylim([0, 2])\n",
    "    plt.ylabel('CO2 (MSE)')\n",
    "    plt.savefig('Countries/{}/Models/{}/mse_trained.png'.format(chosenCountry, modelVersion))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(2)\n",
    "    plotter.plot({'Basic':history}, metric=\"mae\")\n",
    "    plt.ylim([0, 0.5])\n",
    "    plt.ylabel('CO2 (MAE)')\n",
    "    plt.savefig('Countries/{}/Models/{}/mae_trained.png'.format(chosenCountry, modelVersion))\n",
    "    plt.close()\n",
    "    \n",
    "def printRGraph(actual, predicted, error, r2Val, modelVersion, chosenCountry):\n",
    "    plt.figure(3)\n",
    "    a = plt.axes(aspect='equal')\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.xlabel('True Values [CO2]')\n",
    "    plt.ylabel('Predictions [CO2]')\n",
    "    lims = [0, 4]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    _ = plt.plot(lims, lims)\n",
    "    a.set_title('R2: ' + str(r2Val))\n",
    "    plt.savefig('Countries/{}/Models/{}/r2.png'.format(chosenCountry, modelVersion))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(4)\n",
    "    plt.hist(error, bins = 25)\n",
    "    plt.xlabel(\"Prediction Error [CO2]\")\n",
    "    _ = plt.ylabel(\"Count\")\n",
    "    plt.savefig('Countries/{}/Models/{}/error_histogram.png'.format(chosenCountry, modelVersion))\n",
    "    plt.close()\n",
    "    \n",
    "def createAllModels(country, iterations, txtLog):\n",
    "    columns = [\"Model\", \"R2\", \"Epochs\"]\n",
    "    index = range(iterations)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for x in range(iterations):\n",
    "        modelVersion = x\n",
    "        trainedModel, R2, trainedHistory, originalInput, epochsRun = createANN(country, False, True, False, modelVersion, txtLog)\n",
    "        trainedModel.save_weights('Countries/{}/Models/{}/checkpoint_test'.format(country, modelVersion))\n",
    "        data.append([R2, epochsRun])\n",
    "        \n",
    "    modelStats = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelStats.to_csv('Countries/{}/Models/{}ModelStats.csv'.format(country, country))\n",
    "    \n",
    "    modelCharacteristics = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelCharacteristics = modelCharacteristics.describe()\n",
    "    modelCharacteristics.to_csv('Countries/{}/Models/{}ModelChar.csv'.format(country , country))\n",
    "    \n",
    "    return modelStats\n",
    "\n",
    "#Produces csv for all of the countries data for post analysis\n",
    "def produceAllCountryCSV(inputData):\n",
    "    allCountryData = pd.DataFrame(columns=[\"R2\", \"Epochs\", \"Country\"])\n",
    "\n",
    "    for country in inputData:\n",
    "        thisData = pd.read_csv('Countries/{}/Models/{}ModelStats.csv'.format(country, country)) \n",
    "        thisData[\"Country\"] = country\n",
    "        thisData = thisData.drop(['Unnamed: 0'], 1)\n",
    "        frames = [allCountryData, thisData]\n",
    "        allCountryData = pd.concat(frames)\n",
    "\n",
    "    allCountryData.to_csv('Countries/allCountryData.csv')\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Main process to start training the cohort of neural networks\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def runTraining(inputData):\n",
    "\n",
    "    start = timer()\n",
    "    txtLog = open(\"Countries/log.txt\",\"w+\")\n",
    "    startStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    txtLog.write(\"Time started: \" + startStamp +\"\\n \\n\")\n",
    "\n",
    "    for country in inputData:\n",
    "        stamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "        txtLog.write(stamp + \" :: ================================================= \\n\")\n",
    "        txtLog.write(stamp + \" :: \" + country + \" started \\n \\n\")\n",
    "\n",
    "        createAllModels(country, 100, txtLog)\n",
    "\n",
    "        stamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "        txtLog.write(\"\\n \\n\")\n",
    "        txtLog.write(stamp + \" :: \" + country + \" completed\\n\")\n",
    "        print(stamp + \" :: \" + country + \" completed\")\n",
    "        txtLog.write(stamp + \" :: ================================================= \\n\")\n",
    "        txtLog.write(\"  \\n\")\n",
    "\n",
    "    end = timer()\n",
    "    secondsDuration = (end - start)\n",
    "    duration = time.strftime('%H:%M:%S', time.gmtime(secondsDuration))\n",
    "    txtLog.write(\"\")\n",
    "    endStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    txtLog.write(\"Time completed: \" + endStamp +\"\\n\")\n",
    "    txtLog.write(\"Time taken: \" + duration +\"\\n\")\n",
    "    txtLog.close()\n",
    "\n",
    "    print(\"\\nCompleted\")\n",
    "\n",
    "    osCommandString = \"notepad.exe Countries/log.txt\"\n",
    "    os.system(osCommandString)\n",
    "    \n",
    "    produceAllCountryCSV(inputData)\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Create alternate models\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def build_model_retest():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(5, activation=\"relu\", input_dim=8),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimiser = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                 optimizer=optimiser,\n",
    "                 metrics=['mae','mse'])\n",
    "    return model\n",
    "\n",
    "def ANNRetest(chosenCountry, printTraining, printR, snsPlot, modelVersion, structure, structureVal1, structureVal2):\n",
    "    if not os.path.exists('Countries_restoredWeights/{}/{}'.format(chosenCountry, modelVersion)):\n",
    "        os.makedirs('Countries_restoredWeights/{}/{}'.format(chosenCountry, modelVersion))\n",
    "    \n",
    "    allData, all_NP, all_df, trainingDataFull, testData, input_NP, input_df, output_NP, output_df = getAllData(chosenCountry, modelVersion)\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, restore_best_weights=True)\n",
    "    model=build_model_retest()\n",
    "\n",
    "    history = model.fit(\n",
    "        input_NP, \n",
    "        output_NP,\n",
    "        epochs=epochs,\n",
    "        batch_size = batchs,\n",
    "        validation_split = 0.15,\n",
    "        verbose = 0,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    epochsRun = len(history.history['loss'])\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    actualVal, predictedVal, errorVal, normed_test_data = checkTest(chosenCountry, testData, model, modelVersion)\n",
    "    r2Val = math.ceil(r2_score(actualVal, predictedVal)*100)/100\n",
    "    \n",
    "    if printTraining:\n",
    "        printTrainingGraph(history, chosenCountry, modelVersion)\n",
    "\n",
    "    if printR:\n",
    "        printRGraph(actualVal, predictedVal, errorVal, r2Val, modelVersion, chosenCountry)\n",
    "\n",
    "    if snsPlot:\n",
    "        sns.pairplot(input_df)\n",
    "    \n",
    "    stamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    \n",
    "    return model, r2Val, history, input_df, epochsRun\n",
    "\n",
    "def createAllModelsRetest(country, iterations, structureVal1, structureVal2):\n",
    "    structure =\"8-{}-{}-1\".format(structureVal1, structureVal2)\n",
    "    columns = [\"Model\", \"R2\", \"Epochs\"]\n",
    "    index = range(iterations)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for x in range(iterations):\n",
    "        modelVersion = x\n",
    "        trainedModel, R2, trainedHistory, originalInput, epochsRun = ANNRetest(country, False, True, False, modelVersion, structure, structureVal1, structureVal2)\n",
    "        trainedModel.save_weights('Countries_restoredWeights/{}/{}/checkpoint_test'.format(country, modelVersion))\n",
    "        data.append([R2, epochsRun])\n",
    "        \n",
    "    modelStats = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelStats.to_csv('Countries_restoredWeights/{}/{}_ModelStats.csv'.format(country, country))\n",
    "    \n",
    "    modelCharacteristics = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelCharacteristics = modelCharacteristics.describe()\n",
    "    modelCharacteristics.to_csv('Countries_restoredWeights/{}/{}_ModelChar.csv'.format(country, country))\n",
    "    \n",
    "    return modelStats\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Data checks\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def showData(country):\n",
    "    df = getRawData(country)\n",
    "    df = df[20:]\n",
    "    df = df.head(35)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def zeroCheck(inputData):\n",
    "    newNP = np.zeros((1,1))\n",
    "    newDF = pd.DataFrame(data=newNP[1:,1:], dtype='int64')\n",
    "    newDF[\"Country\"] = \"Country\"\n",
    "    for x, country in enumerate(inputData):\n",
    "        df, newNP, wef, train_df, test_df, input_NP, input_df, output_NP, output_df = getAllData(country, \"0\", True, False)\n",
    "        zero_count = (df == 0).astype(int).sum(axis=0)\n",
    "        zero_count = zero_count.to_frame().T\n",
    "        \n",
    "        newDF = newDF.append(zero_count)\n",
    "        newDF.iat[x, 0] = country\n",
    "        \n",
    "    newDF.loc[:,'Total'] = ((newDF.sum(axis=1)/270)*100).round(2)\n",
    "    newDF = newDF.sort_values(by=['Total'])\n",
    "    \n",
    "    return newDF\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Predictions using models\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def prediction(country, predictionInput, modelVersion):\n",
    "    scaler = load('Countries/{}/Models/{}/Input_Scaler.bin'.format(country, modelVersion))\n",
    "    normalised_test = scaler.transform(predictionInput)\n",
    "    \n",
    "    model=build_model()\n",
    "    model.load_weights('Countries/{}/Models/{}/checkpoint_test'.format(country, modelVersion)).expect_partial()\n",
    "    \n",
    "    #test_predictions = model.predict(normalised_test).flatten()\n",
    "    test_predictions = []\n",
    "    \n",
    "    normalised_test = normalised_test.astype('float32')\n",
    "\n",
    "    test_predictions = model(normalised_test)\n",
    "    test_predictions = test_predictions.numpy()\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    outputScaler = load('Countries/{}/Models/{}/Output_Scaler.bin'.format(country, modelVersion))\n",
    "    \n",
    "    del scaler, normalised_test, model\n",
    "    \n",
    "    return outputScaler.inverse_transform(test_predictions)\n",
    "\n",
    "def predictAll(country, predictionInput):\n",
    "    index=range(101)\n",
    "    columns = [\"Results\"]\n",
    "    newDF = pd.DataFrame(index=index, columns=columns)\n",
    "    newDF = newDF.fillna(0)\n",
    "    \n",
    "    for x in range(100):\n",
    "        newDF.loc[x, 0] = prediction(country, predictionInput, x)\n",
    "\n",
    "    newDF.drop(\"Results\", axis=1, inplace=True)\n",
    "\n",
    "    del columns\n",
    "    \n",
    "    return newDF\n",
    "\n",
    "def getYearData(country, year):\n",
    "    df = getRawData(country)\n",
    "    df = df[20:]\n",
    "    df = df.head(35)\n",
    "    df2 = df.loc[year, 'ENR':'URB' ]\n",
    "    df3 = df.loc[year, \"CO2\"]\n",
    "    df2 = df2.to_numpy().reshape(1, -1)\n",
    "    return df2, df3\n",
    "\n",
    "def compareActualHistory(country):\n",
    "    yearsTest = range(1980, 2015)\n",
    "    index = range(101)\n",
    "    columns = [\"Results\"]\n",
    "    resultsDF = pd.DataFrame(index=index, columns=columns)\n",
    "    resultsDF = resultsDF.fillna(0)\n",
    "    \n",
    "    for n in yearsTest:\n",
    "        inputData, CO2 = getYearData(country, str(n))\n",
    "        result = predictAll(country, inputData)\n",
    "        result.loc[100,:] = CO2\n",
    "        resultsDF = pd.concat([resultsDF, result], axis=1)\n",
    "        \n",
    "        print(\"Completed \" + str(n) + \" \")\n",
    "        \n",
    "    resultsDF.drop(\"Results\", axis=1, inplace=True)\n",
    "    resultsDF.columns = yearsTest\n",
    "    resultsDF.to_csv('Projections/History_Check/{}_HistoryCheck.csv'.format(country))\n",
    "    print(\"\\n \" + str(datetime.now().strftime('%H:%M:%S')) + \" Completed \" + country + \"\\n\")\n",
    "    return resultsDF\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Produces histogram to show ranges by country for different structures\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def directoryLooperbyCountry():\n",
    "    newNP = np.zeros((1,1))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for thisStructure in next(os.walk('Structure/'))[1]:\n",
    "    \n",
    "        newNP = np.zeros((101,1))\n",
    "        thisResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "        thisResults[\"0\"]=\" {}\".format(thisStructure)\n",
    "        for country in top6Countries:\n",
    "\n",
    "            thisModelData = pd.read_csv('Structure/{}/{}/{}_ModelStats.csv'.format(thisStructure, country, thisStructure))\n",
    "            thisModelData.columns = [c.replace(' ', '_') for c in thisModelData.columns]\n",
    "            thisModelData.drop(thisModelData.columns[[0,2]], axis=1, inplace=True)\n",
    "\n",
    "            thisModelData.columns = [country]\n",
    "            thisResults = pd.concat([thisResults, thisModelData] , axis=1)\n",
    "\n",
    "        mainResults = pd.concat([mainResults, thisResults])\n",
    "    \n",
    "    mainResults.to_csv('Structure/Structure Comparison.csv'.format(country, country))\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Produces histogram to show ranges by for different structures only\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def directoryLooper():\n",
    "    newNP = np.zeros((1,1))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for thisStructure in next(os.walk('Structure/'))[1]:\n",
    "        \n",
    "        newNP = np.zeros((1,1))\n",
    "        thisResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "\n",
    "        for country in top6Countries:\n",
    "\n",
    "            thisModelData = pd.read_csv('Structure/{}/{}/{}_ModelStats.csv'.format(thisStructure, country, thisStructure))\n",
    "            thisModelData.columns = [c.replace(' ', '_') for c in thisModelData.columns]\n",
    "            thisModelData.drop(thisModelData.columns[[0,2]], axis=1, inplace=True)\n",
    "\n",
    "            thisModelData.columns = [\"Results\"]\n",
    "            thisResults = pd.concat([thisResults, thisModelData])\n",
    "            \n",
    "        thisResults[\"0\"]=\" {}\".format(thisStructure)\n",
    "        \n",
    "        #print(thisStructure + \" mean is \" + str(thisResults[\"Results\"].mean(axis=0)))\n",
    "        mainResults = pd.concat([mainResults, thisResults])\n",
    "        \n",
    "\n",
    "    mainResults.to_csv('Structure/Structure Comparison.csv'.format(country, country))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------\n",
    "#SSP Projections\n",
    "#---------------------------------------------------------------------------------------\n",
    "    \n",
    "SSPS = [\"SSP1\",\"SSP2\",\"SSP3\",\"SSP4\",\"SSP5\"]       \n",
    "projectionYears = [\"2020\", \"2030\", \"2040\", \"2050\"]\n",
    "\n",
    "def countryConverter(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"Code\"].iloc[0]\n",
    "\n",
    "def countryOECD(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"OECD\"].iloc[0]\n",
    "\n",
    "def countryIncome(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"Income\"].iloc[0]\n",
    "\n",
    "def interpolater(year1, year2, y1, y2, year):\n",
    "    x = [year1, year2]\n",
    "    y = [y1, y2]\n",
    "    y_new = np.interp(year, x, y)\n",
    "    \n",
    "    del x, y\n",
    "    \n",
    "    return y_new\n",
    "\n",
    "def getGDPInfo(country, value):\n",
    "    gdparray = [\"GDP (constant LCU)\",\"GDP (current US$)\", \"GDP (current LCU)\", \"PPP conversion factor, GDP (LCU per international $)\",\"GDP deflator: linked series (base year varies by country)\"]\n",
    "    gdpTable = WDI_Data.loc[(WDI_Data['Country_Name'] == country) & WDI_Data['Indicator_Name'].isin(gdparray)]\n",
    "    newValue = (value*1000000000) * gdpTable.loc[(gdpTable['Indicator_Name'] == \"PPP conversion factor, GDP (LCU per international $)\")][\"2005\"].iloc[0]\n",
    "    newValue = ((newValue)/gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (current LCU)\")][\"2005\"].iloc[0])*(gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (constant LCU)\")][\"2005\"].iloc[0])\n",
    "    newValue = (newValue/gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (constant LCU)\")][\"2010\"].iloc[0]) * gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (current US$)\")][\"2010\"].iloc[0]\n",
    "    \n",
    "    del gdpTable, gdparray\n",
    "    \n",
    "    return newValue\n",
    "\n",
    "def industryProjector(country, year, income, thisSSP):\n",
    "    industryData = WDI_Data.loc[(WDI_Data['Country_Name'] == country) & (WDI_Data['Indicator_Name'] == \"Industry (including construction), value added (% of GDP)\")]\n",
    "    industryData=industryData.drop(industryData.columns[0:34],axis=1)\n",
    "    industryData=industryData.drop(['Unnamed:_64'],1)\n",
    "    industryData=industryData.drop(industryData.columns[25:],axis=1)\n",
    "    industryData = industryData.to_numpy().reshape(-1)\n",
    "    x = arange(1990,2015)\n",
    "    indPrediction = interp(year, x, industryData)\n",
    "    rate = predictions[(predictions['INDICATOR'] == \"IND\")& (predictions['TYPE'] == income)][thisSSP].iloc[0]\n",
    "    newRate = ( ( (rate/40)*( 40-(2050-int(year)) ) )/100)+1\n",
    "    \n",
    "    del industryData, rate\n",
    "    \n",
    "    return indPrediction * newRate\n",
    "\n",
    "def createInputData(country, year, thisSSP):\n",
    "    code = countryConverter(country)\n",
    "    income = countryIncome(country)\n",
    "    OECD = countryOECD(country)\n",
    "    \n",
    "    latest, CO2 = getYearData(country, \"2010\")\n",
    "    \n",
    "    thisCountry = SSP_Data.loc[SSP_Data['REGION'] == code]\n",
    "    \n",
    "    population = thisCountry[(thisCountry['VARIABLE'] == \"Population\") & (thisCountry['MODEL'] == \"OECD Env-Growth\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]*1000000\n",
    "    urbanisation = thisCountry[(thisCountry['VARIABLE'] == \"Population|Urban|Share\") & (thisCountry['MODEL'] == \"NCAR\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]\n",
    "    trade = interpolater(2010, 2050, latest[0][6], predictions[(predictions['INDICATOR'] == \"TRD\")][thisSSP].iloc[0] ,int(year))\n",
    "    fdi = interpolater(2010, 2050, latest[0][1], predictions[(predictions['INDICATOR'] == \"FDI\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0] ,int(year))\n",
    "    #rnd = latest[0][4]*(((predictions[(predictions['INDICATOR'] == \"RND\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)+1)**(int(year)-2010))\n",
    "    rnd = latest[0][4]+((latest[0][4]*((predictions[(predictions['INDICATOR'] == \"RND\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)))*(int(year)-2010))\n",
    "    #enr = latest[0][0]*(((predictions[(predictions['INDICATOR'] == \"ENR\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)+1)**(int(year)-2010))\n",
    "    enr = latest[0][0]+((latest[0][0]*((predictions[(predictions['INDICATOR'] == \"ENR\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)))*(int(year)-2010))\n",
    "    \n",
    "    gdp = (getGDPInfo(country, thisCountry[(thisCountry['VARIABLE'] == \"GDP|PPP\") & (thisCountry['MODEL'] == \"OECD Env-Growth\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]))/population\n",
    "    ind = industryProjector(country, year, income, thisSSP)\n",
    "    \n",
    "    thisInputData = [[enr, fdi, gdp, ind, rnd, population, trade, urbanisation]]\n",
    "    \n",
    "    del code, income, OECD, latest, CO2, thisCountry, population, urbanisation, trade, fdi, rnd, enr, gdp, ind\n",
    "    \n",
    "    return thisInputData\n",
    "\n",
    "def SSP_Projection(country):\n",
    "    if not os.path.exists('Projections/Individual/{}/'.format(country)):\n",
    "        os.makedirs('Projections/Individual/{}/'.format(country))\n",
    "    \n",
    "    newNP = np.zeros((1,1))\n",
    "    countrySummary = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for SSP in SSPS:\n",
    "        start = timer()\n",
    "        newNP = np.zeros((1,1))\n",
    "        SSPResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "        for year in projectionYears:\n",
    "            thisResults = predictAll(country, createInputData(country, year, SSP))\n",
    "            SSPResults = pd.concat([SSPResults, thisResults], axis=1)\n",
    "\n",
    "            del thisResults\n",
    "            \n",
    "        SSPResults.columns = projectionYears\n",
    "        characteristics = SSPResults.describe()\n",
    "\n",
    "        SSPStd = characteristics.iloc[2]\n",
    "        ConfInt = (SSPStd/10)*1.96\n",
    "        lowerInt = characteristics.iloc[1] - ConfInt\n",
    "        upperInt = characteristics.iloc[1] + ConfInt\n",
    "        \n",
    "        plotting = pd.concat([lowerInt,characteristics.iloc[1],upperInt], axis=1)\n",
    "        plotting.columns = [SSP+\" lower\", SSP+\" mean\", SSP+\" upper\"]\n",
    "        convertedPlotting = plotting.T\n",
    "        countrySummary = pd.concat([countrySummary, convertedPlotting])\n",
    "        \n",
    "        SSPResults = pd.concat([SSPResults, characteristics])\n",
    "        SSPResults.to_csv('Projections/Individual/{}/{}.csv'.format(country, SSP))\n",
    "\n",
    "        end = timer()\n",
    "        secondsDuration = (end - start)\n",
    "        duration = time.strftime('%H:%M:%S', time.gmtime(secondsDuration))\n",
    "        print(country + \" \" + SSP +  \": \" + duration)\n",
    "        \n",
    "        del start, end, duration, convertedPlotting, plotting, newNP, SSPResults, SSPStd, ConfInt, lowerInt, upperInt, characteristics\n",
    "    \n",
    "    countrySummary.to_csv('Projections/Individual/{}/Summary.csv'.format(country))\n",
    "    \n",
    "    del countrySummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compare the actual vs network projections\n",
    "print(str(datetime.now().strftime('%H:%M:%S'))+ \" Started\")\n",
    "for country in top6Countries:\n",
    "    compareActualHistory(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK2019 = [[2630.53, 0.0790593814371999, 42328.9002575769, 17.4206873962023, 105674, 66836327, 64.2877735623301, 83.652]]\n",
    "UK2018 = [[2757.69, 2.83706174602294, 43043.22782, 17.5188312851163, 94953, 66460344, 62.6190596109507, 83.398]]\n",
    "UK2015 = [[2764.516671, 1.547962, 44974.831877, 18.141621, 57891.0, 65116219.0, 56.683096, 82.626]]\n",
    "USA2017 = [[7547.57012, 1.820076, 59957.725851, 18.20794, 448211.0, 324985539.0, 27.14232, 82.058]]\n",
    "\n",
    "predictionSet = predictAll(\"United Kingdom\", UK2018)\n",
    "predictionSet.to_csv('Projections/History_Check/Adjusted_Input/UK2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:29:37: 1. China\n",
      "China SSP1: 00:00:25\n",
      "China SSP2: 00:00:24\n",
      "China SSP3: 00:00:24\n",
      "China SSP4: 00:00:23\n",
      "China SSP5: 00:00:23\n",
      "\n",
      "\n",
      "15:31:38: 2. United States\n",
      "United States SSP1: 00:00:25\n",
      "United States SSP2: 00:00:23\n",
      "United States SSP3: 00:00:24\n",
      "United States SSP4: 00:00:26\n",
      "United States SSP5: 00:00:25\n",
      "\n",
      "\n",
      "15:33:43: 3. India\n",
      "India SSP1: 00:00:25\n",
      "India SSP2: 00:00:23\n",
      "India SSP3: 00:00:24\n",
      "India SSP4: 00:00:24\n",
      "India SSP5: 00:00:25\n",
      "\n",
      "\n",
      "15:35:46: 4. Russian Federation\n",
      "Russian Federation SSP1: 00:00:40\n",
      "Russian Federation SSP2: 00:00:22\n",
      "Russian Federation SSP3: 00:00:24\n",
      "Russian Federation SSP4: 00:00:22\n",
      "Russian Federation SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:38:00: 5. Japan\n",
      "Japan SSP1: 00:00:25\n",
      "Japan SSP2: 00:00:22\n",
      "Japan SSP3: 00:00:22\n",
      "Japan SSP4: 00:00:22\n",
      "Japan SSP5: 00:00:23\n",
      "\n",
      "\n",
      "15:39:57: 6. Canada\n",
      "Canada SSP1: 00:00:24\n",
      "Canada SSP2: 00:00:22\n",
      "Canada SSP3: 00:00:22\n",
      "Canada SSP4: 00:00:22\n",
      "Canada SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:41:52: 7. Germany\n",
      "Germany SSP1: 00:00:26\n",
      "Germany SSP2: 00:00:22\n",
      "Germany SSP3: 00:00:22\n",
      "Germany SSP4: 00:00:22\n",
      "Germany SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:43:49: 8. Korea, Rep.\n",
      "Korea, Rep. SSP1: 00:00:23\n",
      "Korea, Rep. SSP2: 00:00:22\n",
      "Korea, Rep. SSP3: 00:00:22\n",
      "Korea, Rep. SSP4: 00:00:22\n",
      "Korea, Rep. SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:45:43: 9. Brazil\n",
      "Brazil SSP1: 00:00:25\n",
      "Brazil SSP2: 00:00:22\n",
      "Brazil SSP3: 00:00:23\n",
      "Brazil SSP4: 00:00:22\n",
      "Brazil SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:47:40: 10. France\n",
      "France SSP1: 00:00:24\n",
      "France SSP2: 00:00:22\n",
      "France SSP3: 00:00:22\n",
      "France SSP4: 00:00:22\n",
      "France SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:49:34: 11. Saudi Arabia\n",
      "Saudi Arabia SSP1: 00:00:23\n",
      "Saudi Arabia SSP2: 00:00:22\n",
      "Saudi Arabia SSP3: 00:00:22\n",
      "Saudi Arabia SSP4: 00:00:22\n",
      "Saudi Arabia SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:51:28: 12. United Kingdom\n",
      "United Kingdom SSP1: 00:00:24\n",
      "United Kingdom SSP2: 00:00:22\n",
      "United Kingdom SSP3: 00:00:22\n",
      "United Kingdom SSP4: 00:00:22\n",
      "United Kingdom SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:53:23: 13. Pakistan\n",
      "Pakistan SSP1: 00:00:24\n",
      "Pakistan SSP2: 00:00:22\n",
      "Pakistan SSP3: 00:00:22\n",
      "Pakistan SSP4: 00:00:22\n",
      "Pakistan SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:55:18: 14. Mexico\n",
      "Mexico SSP1: 00:00:23\n",
      "Mexico SSP2: 00:00:22\n",
      "Mexico SSP3: 00:00:22\n",
      "Mexico SSP4: 00:00:22\n",
      "Mexico SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:57:12: 15. Iran, Islamic Rep.\n",
      "Iran, Islamic Rep. SSP1: 00:00:24\n",
      "Iran, Islamic Rep. SSP2: 00:00:22\n",
      "Iran, Islamic Rep. SSP3: 00:00:22\n",
      "Iran, Islamic Rep. SSP4: 00:00:22\n",
      "Iran, Islamic Rep. SSP5: 00:00:22\n",
      "\n",
      "\n",
      "15:59:07: 16. Turkey\n",
      "Turkey SSP1: 00:00:23\n",
      "Turkey SSP2: 00:00:22\n",
      "Turkey SSP3: 00:00:22\n",
      "Turkey SSP4: 00:00:22\n",
      "Turkey SSP5: 00:00:22\n",
      "\n",
      "\n",
      "16:01:00: 17. Italy\n",
      "Italy SSP1: 00:00:23\n",
      "Italy SSP2: 00:00:22\n",
      "Italy SSP3: 00:00:22\n",
      "Italy SSP4: 00:00:22\n",
      "Italy SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:02:56: 18. Spain\n",
      "Spain SSP1: 00:00:24\n",
      "Spain SSP2: 00:00:22\n",
      "Spain SSP3: 00:00:22\n",
      "Spain SSP4: 00:00:22\n",
      "Spain SSP5: 00:00:22\n",
      "\n",
      "\n",
      "16:04:51: 19. Indonesia\n",
      "Indonesia SSP1: 00:00:24\n",
      "Indonesia SSP2: 00:00:22\n",
      "Indonesia SSP3: 00:00:23\n",
      "Indonesia SSP4: 00:00:22\n",
      "Indonesia SSP5: 00:00:22\n",
      "\n",
      "\n",
      "16:06:46: 20. Australia\n",
      "Australia SSP1: 00:00:23\n",
      "Australia SSP2: 00:00:22\n",
      "Australia SSP3: 00:00:22\n",
      "Australia SSP4: 00:00:22\n",
      "Australia SSP5: 00:00:22\n",
      "\n",
      "\n",
      "16:08:39: 21. South Africa\n",
      "South Africa SSP1: 00:00:23\n",
      "South Africa SSP2: 00:00:22\n",
      "South Africa SSP3: 00:00:26\n",
      "South Africa SSP4: 00:00:24\n",
      "South Africa SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:10:40: 22. Vietnam\n",
      "Vietnam SSP1: 00:00:26\n",
      "Vietnam SSP2: 00:00:24\n",
      "Vietnam SSP3: 00:00:23\n",
      "Vietnam SSP4: 00:00:23\n",
      "Vietnam SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:12:43: 23. Egypt, Arab Rep.\n",
      "Egypt, Arab Rep. SSP1: 00:00:25\n",
      "Egypt, Arab Rep. SSP2: 00:00:23\n",
      "Egypt, Arab Rep. SSP3: 00:00:24\n",
      "Egypt, Arab Rep. SSP4: 00:00:23\n",
      "Egypt, Arab Rep. SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:14:44: 24. Thailand\n",
      "Thailand SSP1: 00:00:25\n",
      "Thailand SSP2: 00:00:23\n",
      "Thailand SSP3: 00:00:23\n",
      "Thailand SSP4: 00:00:24\n",
      "Thailand SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:16:44: 25. Argentina\n",
      "Argentina SSP1: 00:00:25\n",
      "Argentina SSP2: 00:00:23\n",
      "Argentina SSP3: 00:00:23\n",
      "Argentina SSP4: 00:00:23\n",
      "Argentina SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:18:44: 26. Nigeria\n",
      "Nigeria SSP1: 00:00:25\n",
      "Nigeria SSP2: 00:00:23\n",
      "Nigeria SSP3: 00:00:23\n",
      "Nigeria SSP4: 00:00:25\n",
      "Nigeria SSP5: 00:00:24\n",
      "\n",
      "\n",
      "16:20:45: 27. Poland\n",
      "Poland SSP1: 00:00:25\n",
      "Poland SSP2: 00:00:24\n",
      "Poland SSP3: 00:00:23\n",
      "Poland SSP4: 00:00:23\n",
      "Poland SSP5: 00:00:24\n",
      "\n",
      "\n",
      "16:22:46: 28. Malaysia\n",
      "Malaysia SSP1: 00:00:25\n",
      "Malaysia SSP2: 00:00:23\n",
      "Malaysia SSP3: 00:00:23\n",
      "Malaysia SSP4: 00:00:24\n",
      "Malaysia SSP5: 00:00:23\n",
      "\n",
      "\n",
      "16:24:46: 29. Venezuela, RB\n",
      "Venezuela, RB SSP1: 00:00:25\n",
      "Venezuela, RB SSP2: 00:00:23\n",
      "Venezuela, RB SSP3: 00:00:23\n",
      "Venezuela, RB SSP4: 00:00:23\n",
      "Venezuela, RB SSP5: 00:00:24\n",
      "\n",
      "\n",
      "16:26:47: 30. Congo, Dem. Rep.\n",
      "Congo, Dem. Rep. SSP1: 00:00:25\n",
      "Congo, Dem. Rep. SSP2: 00:00:23\n",
      "Congo, Dem. Rep. SSP3: 00:00:23\n",
      "Congo, Dem. Rep. SSP4: 00:00:24\n",
      "Congo, Dem. Rep. SSP5: 00:00:23\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, country in enumerate(top30Countries):\n",
    "    startStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    print(startStamp + \": \" + str(x+1) + \". \" + country)\n",
    "    SSP_Projection(country)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalerTest(country, predictionInput, modelVersion):\n",
    "    scaler = load('Countries/{}/Models/{}/Input_Scaler.bin'.format(country, modelVersion))\n",
    "    normalised_test = scaler.transform(predictionInput)\n",
    "    \n",
    "    model=build_model()\n",
    "    model.load_weights('Countries/{}/Models/{}/checkpoint_test'.format(country, modelVersion)).expect_partial()\n",
    "    \n",
    "    #test_predictions = model.predict(normalised_test).flatten()\n",
    "    test_predictions = []\n",
    "    \n",
    "    normalised_test = normalised_test.astype('float32')\n",
    "\n",
    "    test_predictions = model(normalised_test)\n",
    "    test_predictions = test_predictions.numpy()\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    outputScaler = load('Countries/{}/Models/{}/Output_Scaler.bin'.format(country, modelVersion))\n",
    "    \n",
    "    #del scaler, normalised_test, model\n",
    "    \n",
    "    return normalised_test, predictionInput\n",
    "\n",
    "UK2015 = [[2764.516671, 1.547962, 44974.831877, 18.141621, 57891.0, 65116219.0, 56.683096, 82.626]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.8919036 , -0.66108465,  1.3425224 ,  0.23228668,  3.043277  ,\n",
       "          2.4578793 ,  1.1025933 ,  2.6209843 ]], dtype=float32),\n",
       " [[2764.516671,\n",
       "   1.547962,\n",
       "   44974.831877,\n",
       "   18.141621,\n",
       "   57891.0,\n",
       "   65116219.0,\n",
       "   56.683096,\n",
       "   82.626]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerTest(\"United Kingdom\", UK2015, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
