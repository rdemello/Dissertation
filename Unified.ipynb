{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_rows = 4000\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import arange\n",
    "from numpy import interp\n",
    "import math\n",
    "import os, psutil\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import time\n",
    "import memory_profiler\n",
    "from memory_profiler import profile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import dump, load\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "WDI = pd.read_csv('02_Data/WDIData.csv')  \n",
    "WDI.columns = [c.replace(' ', '_') for c in WDI.columns]\n",
    "\n",
    "hdi_data = pd.read_csv('02_Data/HDI.csv')  \n",
    "hdi_data.columns = [c.replace(' ', '_') for c in hdi_data.columns]\n",
    "\n",
    "pv_data = pd.read_csv('02_Data/PV.csv')  \n",
    "pv_data.columns = [c.replace(' ', '_') for c in pv_data.columns]\n",
    "\n",
    "hyd_data = pd.read_csv('02_Data/HYD.csv')  \n",
    "hyd_data.columns = [c.replace(' ', '_') for c in hyd_data.columns]\n",
    "\n",
    "countrycode = pd.read_csv('02_Data/COUNTRYCODES.csv')  \n",
    "countrycode.columns = [c.replace(' ', '_') for c in countrycode.columns]\n",
    "\n",
    "predictions = pd.read_csv('02_Data/PREDICTIONS.csv')  \n",
    "predictions.columns = [c.replace(' ', '_') for c in predictions.columns]\n",
    "\n",
    "SSP_Data = pd.read_csv('02_Data/SSP.csv')  \n",
    "SSP_Data.columns = [c.replace(' ', '_') for c in SSP_Data.columns]\n",
    "\n",
    "WDI_Data = pd.read_csv('02_Data/WDIData.csv')  \n",
    "WDI_Data.columns = [c.replace(' ', '_') for c in WDI_Data.columns]\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPath = \"Groupings/IMF/Emerging\"\n",
    "titles = [\"ENR\", \"FDI\", \"GDP\", \"IND\", \"RND\", \"POP\", \"TRD\", \"URB\", \"CO2\"]\n",
    "epochs = 1000\n",
    "batchs = 32\n",
    "testCountries = [\"United Kingdom\", \"France\", \"Germany\"]\n",
    "top30Countries = [\"China\", \"United States\",\"India\", \"Russian Federation\", \"Japan\", \n",
    "                  \"Canada\", \"Germany\", \"Korea, Rep.\", \"Brazil\", \"France\", \"Saudi Arabia\", \n",
    "                  \"United Kingdom\", \"Pakistan\", \"Mexico\", \"Iran, Islamic Rep.\", \"Turkey\", \"Italy\", \"Spain\", \n",
    "                  \"Indonesia\", \"Australia\", \"South Africa\", \"Vietnam\", \"Egypt, Arab Rep.\", \n",
    "                  \"Thailand\", \"Argentina\",\"Nigeria\",\"Poland\", \"Malaysia\", \"Venezuela, RB\", \"Congo, Dem. Rep.\"]\n",
    "\n",
    "UNDeveloped = [\"United States\", \"Russian Federation\", \"Japan\", \n",
    "                  \"Canada\", \"Germany\", \"Korea, Rep.\", \"France\", \"Saudi Arabia\", \n",
    "                  \"United Kingdom\", \"Turkey\", \"Italy\", \"Spain\", \n",
    "                  \"Australia\", \"Argentina\",\"Poland\"]\n",
    "\n",
    "IMFAdvanced = [\"United States\", \"Japan\", \n",
    "                  \"Canada\", \"Germany\", \"Korea, Rep.\", \"France\",  \n",
    "                  \"United Kingdom\", \"Italy\", \"Spain\", \n",
    "                  \"Australia\"]\n",
    "\n",
    "UNDeveloping = [x for x in top30Countries if x not in UNDeveloped]\n",
    "IMFEmerging = [x for x in top30Countries if x not in IMFAdvanced]\n",
    "\n",
    "def getPower(inputData, country):\n",
    "    power = inputData.loc[inputData['country'] == country]\n",
    "    return power.iloc[0][\"power\"]\n",
    "\n",
    "def getHDI(inputData, country):\n",
    "    inputData[\"Country\"] = inputData[\"Country\"].str.lstrip()\n",
    "    hdi = inputData.loc[inputData['Country'] == country]\n",
    "    hdi = hdi.transpose()\n",
    "    hdi = hdi[1:26]\n",
    "    hdi.columns = [\"HDI\"]\n",
    "    return hdi\n",
    "\n",
    "def getArea(country):\n",
    "    country_data = WDI.loc[WDI['Country_Name'] == country]\n",
    "    country_data = country_data.fillna(0)\n",
    "    country_data = country_data.drop(['Country_Code','Country_Name','Unnamed:_64','Indicator_Code'], 1)\n",
    "    \n",
    "    AREA = country_data.loc[country_data['Indicator_Name'] == \"Surface area (sq. km)\"]\n",
    "    AREA=AREA.transpose()\n",
    "    AREA= AREA.iloc[1:].astype(float)\n",
    "    AREA.columns = [\"Country\"]\n",
    "    \n",
    "    return AREA.iloc[[20]].iloc[0]\n",
    "    \n",
    "def getRawData(country):\n",
    "    country_data = WDI.loc[WDI['Country_Name'] == country]\n",
    "    country_data = country_data.fillna(0)\n",
    "    country_data = country_data.drop(['Country_Code','Country_Name','Unnamed:_64','Indicator_Code'], 1)\n",
    "    #Total greenhouse gas emissions (kt of CO2 equivalent)\n",
    "    #Fossil fuel energy consumption (% of total)\n",
    "    ENR = country_data.loc[country_data['Indicator_Name'] == \"Energy use (kg of oil equivalent per capita)\"] \n",
    "    FDI = country_data.loc[country_data['Indicator_Name'] == \"Foreign direct investment, net inflows (% of GDP)\"] \n",
    "    GDP = country_data.loc[country_data['Indicator_Name'] == \"GDP per capita (current US$)\"] \n",
    "    IND = country_data.loc[country_data['Indicator_Name'] == \"Industry (including construction), value added (% of GDP)\"] \n",
    "    RND = country_data.loc[country_data['Indicator_Name'] == \"Trademark applications, total\"] \n",
    "    POP = country_data.loc[country_data['Indicator_Name'] == \"Population, total\"]\n",
    "    TRD = country_data.loc[country_data['Indicator_Name'] == \"Trade (% of GDP)\"] \n",
    "    URB = country_data.loc[country_data['Indicator_Name'] == \"Urban population (% of total population)\"]\n",
    "    CO2 = country_data.loc[country_data['Indicator_Name'] == \"CO2 intensity (kg per kg of oil equivalent energy use)\"]\n",
    "\n",
    "    AREA = country_data.loc[country_data['Indicator_Name'] == \"Surface area (sq. km)\"]\n",
    "    \n",
    "    AREA=AREA.transpose()\n",
    "    AREA= AREA.iloc[1:].astype(float)\n",
    "    AREA.columns = [\"Country\"]\n",
    "    POP=POP.transpose()\n",
    "    POP= POP.iloc[1:].astype(float)\n",
    "    POP.columns = [\"Country\"]\n",
    "    DEN = POP/AREA\n",
    "    DEN=DEN.transpose()\n",
    "                            \n",
    "    ALL = [ENR, FDI, GDP, IND, RND, DEN, TRD, URB, CO2]\n",
    "    INPUT = pd.concat(ALL)\n",
    "    INPUT = INPUT.transpose()\n",
    "    INPUT.columns = titles\n",
    "    INPUT = INPUT.iloc[31:56]\n",
    "    \n",
    "    INPUT[\"PV\"]=getPower(pv_data, country)\n",
    "    INPUT[\"HYD\"]=getPower(hyd_data, country)\n",
    "\n",
    "    RAWHYD = INPUT[\"HYD\"]\n",
    "    HYD=RAWHYD/AREA[\"Country\"]\n",
    "\n",
    "    INPUT[\"HYD\"] = HYD\n",
    "    INPUT[\"HDI\"] = getHDI(hdi_data, country)\n",
    "    INPUT = INPUT.astype(float)\n",
    "    return INPUT\n",
    "\n",
    "def getAllData(modelVersion, noZeroes, dumpScaler, dataInputs):\n",
    "    \n",
    "    newNP = np.zeros((1,1))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for country in dataInputs:\n",
    "        df = getRawData(country)\n",
    "        mainResults = pd.concat([mainResults, df])\n",
    "\n",
    "    #return mainResults\n",
    "        \n",
    "    if noZeroes:\n",
    "        mainResults = mainResults.astype(float)\n",
    "        mainResults.index = mainResults.index.map(int)\n",
    "\n",
    "        for column in df:\n",
    "            if(mainResults[column] == 0).any():\n",
    "                mainResults[column].replace(0, np.nan, inplace=True)\n",
    "                mainResults[column] = mainResults[column].interpolate(method=\"linear\", limit_direction=\"both\", limit_area=\"inside\")\n",
    "                mainResults[column]  = mainResults[column].fillna(0)\n",
    "\n",
    "    mainResults  =  mainResults.reset_index(drop=True)\n",
    "    train_df = mainResults.sample(frac=0.85,random_state=0)\n",
    "    test_df = mainResults.drop(train_df.index)\n",
    "\n",
    "    input_df = train_df.drop('CO2', axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    input_NP = input_df.to_numpy()\n",
    "    input_NP = scaler.fit_transform(input_NP)\n",
    "    if dumpScaler:\n",
    "        dump(scaler, '{}/Models/{}/Input_Scaler.bin'.format(currentPath, modelVersion), compress=True)\n",
    "\n",
    "    output_df = train_df.drop([\"ENR\", \"FDI\", \"GDP\", \"IND\", \"RND\", \"POP\", \"TRD\", \"URB\", \"HDI\", \"HYD\", \"PV\"], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    output_NP = output_df.to_numpy()\n",
    "    output_NP = scaler.fit_transform(output_NP)\n",
    "    if dumpScaler:\n",
    "        dump(scaler, '{}/Models/{}/Output_Scaler.bin'.format(currentPath, modelVersion), compress=True)\n",
    "    \n",
    "    \n",
    "    return df, newNP, mainResults, train_df, test_df, input_NP, input_df, output_NP, output_df\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(5, activation=\"relu\", input_dim=11),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimiser = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                 optimizer=optimiser,\n",
    "                 metrics=['mae','mse'])\n",
    "    return model\n",
    "\n",
    "def checkTest(testData, model, modelVersion):\n",
    "    if 'CO2' in testData.columns:\n",
    "        test_labels = testData.pop('CO2')\n",
    "    scaler = load('{}/Models/{}/Input_Scaler.bin'.format(currentPath, modelVersion))\n",
    "    normed_test_data  = scaler.transform(testData)\n",
    "\n",
    "    test_predictions = model.predict(normed_test_data).flatten()\n",
    "    outputScaler = load('{}/Models/{}/Output_Scaler.bin'.format(currentPath, modelVersion))\n",
    "    finalOutput = outputScaler.inverse_transform(test_predictions)\n",
    "\n",
    "    error = finalOutput - test_labels\n",
    "    \n",
    "    return test_labels, finalOutput, error, normed_test_data\n",
    "\n",
    "def createANN(printTraining, printR, snsPlot, modelVersion, txtLog, inputData):\n",
    "    if not os.path.exists('{}/Models/{}'.format(currentPath, modelVersion)):\n",
    "        os.makedirs('{}/Models/{}'.format(currentPath, modelVersion))\n",
    "    \n",
    "    allData, all_NP, all_df, trainingDataFull, testData, input_NP, input_df, output_NP, output_df = getAllData(modelVersion, True, True, inputData)\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model=build_model()\n",
    "    \n",
    "    history = model.fit(\n",
    "        input_NP, \n",
    "        output_NP,\n",
    "        epochs=epochs,\n",
    "        batch_size = batchs,\n",
    "        validation_split = 0.15,\n",
    "        verbose = 0,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    epochsRun = len(history.history['loss'])\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    hist.to_csv('{}/Models/{}/history.csv'.format(currentPath, modelVersion))\n",
    "    \n",
    "    \n",
    "    actualVal, predictedVal, errorVal, normed_test_data = checkTest(testData, model, modelVersion)\n",
    "    r2Val = math.ceil(r2_score(actualVal, predictedVal)*100)/100\n",
    "    \n",
    "    if printTraining:\n",
    "        printTrainingGraph(history, modelVersion)\n",
    "\n",
    "    if printR:\n",
    "        printRGraph(actualVal, predictedVal, errorVal, r2Val, modelVersion)\n",
    "\n",
    "    if snsPlot:\n",
    "        sns.pairplot(input_df)\n",
    "    \n",
    "    stamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    \n",
    "    if r2Val < 0:\n",
    "        txtLog.write(stamp + \" :: NEGATIVE R2: \"+ str(r2Val)+\" in model \"+ str(modelVersion) +\" \\n\")\n",
    "        r2values = pd.DataFrame({'Actual': actualVal, 'Predicted': predictedVal}, columns=['Actual', 'Predicted'])\n",
    "        r2values.to_csv('{}/Models/{}/r2Values.csv'.format(currentPath, modelVersion))\n",
    "        \n",
    "    return model, r2Val, history, input_df, epochsRun\n",
    "\n",
    "def printTrainingGraph(history, modelVersion):\n",
    "    plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "    plt.figure(1)\n",
    "    plotter.plot({'Basic':history}, metric=\"mse\")\n",
    "    plt.ylim([0, 2])\n",
    "    plt.ylabel('CO2 (MSE)')\n",
    "    plt.savefig('{}/Models/{}/mse_trained.png'.format(currentPath, modelVersion))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(2)\n",
    "    plotter.plot({'Basic':history}, metric=\"mae\")\n",
    "    plt.ylim([0, 0.5])\n",
    "    plt.ylabel('CO2 (MAE)')\n",
    "    plt.savefig('{}/Models/{}/mae_trained.png'.format(currentPath,modelVersion))\n",
    "    plt.close()\n",
    "    \n",
    "def printRGraph(actual, predicted, error, r2Val, modelVersion):\n",
    "    plt.figure(3)\n",
    "    a = plt.axes(aspect='equal')\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.xlabel('True Values [CO2]')\n",
    "    plt.ylabel('Predictions [CO2]')\n",
    "    lims = [0, 4]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    _ = plt.plot(lims, lims)\n",
    "    a.set_title('R2: ' + str(r2Val))\n",
    "    plt.savefig('{}/Models/{}/r2.png'.format(currentPath, modelVersion))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(4)\n",
    "    plt.hist(error, bins = 25)\n",
    "    plt.xlabel(\"Prediction Error [CO2]\")\n",
    "    _ = plt.ylabel(\"Count\")\n",
    "    plt.savefig('{}/Models/{}/error_histogram.png'.format(currentPath, modelVersion))\n",
    "    plt.close()\n",
    "    \n",
    "def createAllModels(iterations, txtLog, inputData):\n",
    "    columns = [\"Model\", \"R2\", \"Epochs\"]\n",
    "    index = range(iterations)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for x in range(iterations):\n",
    "        modelVersion = x\n",
    "        trainedModel, R2, trainedHistory, originalInput, epochsRun = createANN(False, True, False, modelVersion, txtLog,inputData)\n",
    "        trainedModel.save_weights('{}/Models/{}/checkpoint_test'.format(currentPath, modelVersion))\n",
    "        data.append([R2, epochsRun])\n",
    "        \n",
    "    modelStats = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelStats.to_csv('{}/Models/ModelStats.csv'.format(currentPath))\n",
    "    \n",
    "    modelCharacteristics = pd.DataFrame(data, columns=['R2', 'Epochs'])\n",
    "    modelCharacteristics = modelCharacteristics.describe()\n",
    "    modelCharacteristics.to_csv('{}/Models/ModelChar.csv'.format(currentPath))\n",
    "    \n",
    "    return modelStats\n",
    "\n",
    "#Produces csv for all of the countries data for post analysis\n",
    "def produceAllCountryCSV(inputData):\n",
    "    allCountryData = pd.DataFrame(columns=[\"R2\", \"Epochs\", \"Country\"])\n",
    "\n",
    "    for country in inputData:\n",
    "        thisData = pd.read_csv('{}/Models/ModelStats.csv'.format(currentPath)) \n",
    "        thisData[\"Country\"] = country\n",
    "        thisData = thisData.drop(['Unnamed: 0'], 1)\n",
    "        frames = [allCountryData, thisData]\n",
    "        allCountryData = pd.concat(frames)\n",
    "\n",
    "    allCountryData.to_csv('{}/allData.csv'.format(currentPath))\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Main process to start training the cohort of neural networks\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def runTraining(inputData):\n",
    "\n",
    "    start = timer()\n",
    "    txtLog = open(\"{}/log.txt\".format(currentPath),\"w+\")\n",
    "    startStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    txtLog.write(\"Time started: \" + startStamp +\"\\n \\n\")\n",
    "\n",
    "    createAllModels(100, txtLog, inputData)\n",
    "\n",
    "    end = timer()\n",
    "    secondsDuration = (end - start)\n",
    "    duration = time.strftime('%H:%M:%S', time.gmtime(secondsDuration))\n",
    "    txtLog.write(\"\")\n",
    "    endStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    txtLog.write(\"Time completed: \" + endStamp +\"\\n\")\n",
    "    txtLog.write(\"Time taken: \" + duration +\"\\n\")\n",
    "    txtLog.close()\n",
    "\n",
    "    print(\"\\nCompleted\")\n",
    "\n",
    "    osCommandString = \"notepad.exe {}/log.txt\".format(currentPath)\n",
    "    os.system(osCommandString)\n",
    "    \n",
    "    #produceAllCountryCSV(inputData)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Data checks\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def zeroCheck(inputData):\n",
    "    newNP = np.zeros((1,1))\n",
    "    newDF = pd.DataFrame(data=newNP[1:,1:], dtype='int64')\n",
    "    newDF[\"Country\"] = \"Country\"\n",
    "    for x, country in enumerate(inputData):\n",
    "        df = getRawData(country)\n",
    "        zero_count = (df == 0).astype(int).sum(axis=0)\n",
    "        zero_count = zero_count.to_frame().T\n",
    "        \n",
    "        newDF = newDF.append(zero_count)\n",
    "        newDF.iat[x, 0] = country\n",
    "        \n",
    "    newDF.loc[:,'Total'] = ((newDF.sum(axis=1)/270)*100).round(2)\n",
    "    newDF = newDF.sort_values(by=['Total'])\n",
    "    \n",
    "    return newDF\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Predictions using models\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def prediction(predictionInput, modelVersion):\n",
    "    \n",
    "    scaler = load('{}/Models/{}/Input_Scaler.bin'.format(currentPath, modelVersion))\n",
    "    normalised_test = scaler.transform(predictionInput)\n",
    "    \n",
    "    model=build_model()\n",
    "    model.load_weights('{}/Models/{}/checkpoint_test'.format(currentPath, modelVersion)).expect_partial()\n",
    "    \n",
    "    test_predictions = []\n",
    "    #test_predictions = model.predict(normalised_test).flatten()\n",
    "    \n",
    "    normalised_test = normalised_test.astype('float32')\n",
    "\n",
    "    test_predictions = model(normalised_test)\n",
    "    test_predictions = test_predictions.numpy()\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    outputScaler = load('{}/Models/{}/Output_Scaler.bin'.format(currentPath, modelVersion))\n",
    "\n",
    "    del scaler, normalised_test, model\n",
    "\n",
    "    return outputScaler.inverse_transform(test_predictions)\n",
    "\n",
    "def predictAll(predictionInput):\n",
    "    \n",
    "    index=range(101)\n",
    "    columns = [\"Results\"]\n",
    "    \n",
    "    newDF = pd.DataFrame(index=index, columns=columns)\n",
    "    newDF = newDF.fillna(0)\n",
    "    \n",
    "    for x in range(100):\n",
    "        newDF.loc[x, 0] = prediction(predictionInput, x) \n",
    "    \n",
    "    newDF.drop(\"Results\", axis=1, inplace=True)\n",
    "    \n",
    "    del columns\n",
    "    \n",
    "    return newDF\n",
    "\n",
    "def getYearData(country, year):\n",
    "    df = getRawData(country)\n",
    "    df2 = df.loc[year, 'ENR':'HDI' ]\n",
    "    df2 = df2.drop(\"CO2\")\n",
    "    df3 = df.loc[year, \"CO2\"]\n",
    "    df2 = df2.to_numpy().reshape(1, -1)\n",
    "    return df2, df3\n",
    "\n",
    "def compareActualHistory(country):\n",
    "    yearsTest = range(1990, 2015)\n",
    "    index = range(101)\n",
    "    columns = [\"Results\"]\n",
    "    resultsDF = pd.DataFrame(index=index, columns=columns)\n",
    "    resultsDF = resultsDF.fillna(0)\n",
    "    \n",
    "    for n in yearsTest:\n",
    "        inputData, CO2 = getYearData(country, str(n))\n",
    "        result = predictAll(inputData)\n",
    "        result.loc[100,:] = CO2\n",
    "        resultsDF = pd.concat([resultsDF, result], axis=1)\n",
    "        \n",
    "        print(\"Completed \" + str(n) + \" \")\n",
    "        \n",
    "    resultsDF.drop(\"Results\", axis=1, inplace=True)\n",
    "    resultsDF.columns = yearsTest\n",
    "    resultsDF.to_csv('{}/Projections/{}_HistoryCheck.csv'.format(currentPath, country))\n",
    "    print(\"\\n \" + str(datetime.now().strftime('%H:%M:%S')) + \" Completed \" + country + \"\\n\")\n",
    "    return resultsDF\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Produces histogram to show ranges by country for different structures\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def directoryLooperbyCountry():\n",
    "    newNP = np.zeros((1,1))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for thisStructure in next(os.walk('Structure/'))[1]:\n",
    "    \n",
    "        newNP = np.zeros((101,1))\n",
    "        thisResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "        thisResults[\"0\"]=\" {}\".format(thisStructure)\n",
    "        for country in top6Countries:\n",
    "\n",
    "            thisModelData = pd.read_csv('Structure/{}/{}/{}_ModelStats.csv'.format(thisStructure, country, thisStructure))\n",
    "            thisModelData.columns = [c.replace(' ', '_') for c in thisModelData.columns]\n",
    "            thisModelData.drop(thisModelData.columns[[0,2]], axis=1, inplace=True)\n",
    "\n",
    "            thisModelData.columns = [country]\n",
    "            thisResults = pd.concat([thisResults, thisModelData] , axis=1)\n",
    "\n",
    "        mainResults = pd.concat([mainResults, thisResults])\n",
    "    \n",
    "    mainResults.to_csv('Structure/Structure Comparison.csv'.format(country, country))\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Produces histogram to show ranges by for different structures only\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def directoryLooper():\n",
    "    newNP = np.zeros((1,1))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for thisStructure in next(os.walk('Structure/'))[1]:\n",
    "        \n",
    "        newNP = np.zeros((1,1))\n",
    "        thisResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "\n",
    "        for country in top6Countries:\n",
    "\n",
    "            thisModelData = pd.read_csv('Structure/{}/{}/{}_ModelStats.csv'.format(thisStructure, country, thisStructure))\n",
    "            thisModelData.columns = [c.replace(' ', '_') for c in thisModelData.columns]\n",
    "            thisModelData.drop(thisModelData.columns[[0,2]], axis=1, inplace=True)\n",
    "\n",
    "            thisModelData.columns = [\"Results\"]\n",
    "            thisResults = pd.concat([thisResults, thisModelData])\n",
    "            \n",
    "        thisResults[\"0\"]=\" {}\".format(thisStructure)\n",
    "        \n",
    "        #print(thisStructure + \" mean is \" + str(thisResults[\"Results\"].mean(axis=0)))\n",
    "        mainResults = pd.concat([mainResults, thisResults])\n",
    "        \n",
    "\n",
    "    mainResults.to_csv('Structure/Structure Comparison.csv'.format(country, country))\n",
    "\n",
    "    \n",
    "#---------------------------------------------------------------------------------------\n",
    "#HDI Functions\n",
    "#---------------------------------------------------------------------------------------\n",
    "    \n",
    "def HDICalculate():\n",
    "    \n",
    "    newNP = np.zeros((1,3))\n",
    "    mainResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "    mainResults.columns=[\"Value\",\"Change\"]\n",
    "    for x in range(206):\n",
    "        thisrow = hdi_data.loc[ x , : ]\n",
    "        thisrow=thisrow[1:31].astype(float)\n",
    "        difference = thisrow.diff()\n",
    "        results = pd.concat([thisrow, difference], axis=1)\n",
    "        results.columns=[\"Value\",\"Change\"]\n",
    "\n",
    "        mainResults = pd.concat([mainResults, results])\n",
    "        \n",
    "    mainResults.fillna(0)\n",
    "    mainResults.to_csv('02_Data/HDI_adjusted.csv')\n",
    "    \n",
    "def HDIProjector(h, x, k):\n",
    "    y=int(x)-2010\n",
    "    for i in range(y):\n",
    "        h = h*(1-k)+k\n",
    "    return h\n",
    "    \n",
    "\n",
    "SSPS = [\"SSP1\",\"SSP2\",\"SSP3\",\"SSP4\",\"SSP5\"]  \n",
    " \n",
    "projectionYears = [\"2020\", \"2030\", \"2040\", \"2050\"]\n",
    "nextYears = [\"2060\", \"2070\", \"2080\", \"2090\", \"2100\"]\n",
    "allYears = projectionYears + nextYears\n",
    "\n",
    "def countryConverter(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"Code\"].iloc[0]\n",
    "\n",
    "def countryOECD(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"OECD\"].iloc[0]\n",
    "\n",
    "def countryIncome(country):\n",
    "    return countrycode.loc[countrycode[\"Country\"]==country, \"Income\"].iloc[0]\n",
    "\n",
    "def interpolater(year1, year2, y1, y2, year):\n",
    "    x = [year1, year2]\n",
    "    y = [y1, y2]\n",
    "    y_new = np.interp(year, x, y)\n",
    "    \n",
    "    del x, y\n",
    "    \n",
    "    return y_new\n",
    "\n",
    "def getGDPInfo(country, value):\n",
    "    gdparray = [\"GDP (constant LCU)\",\"GDP (current US$)\", \"GDP (current LCU)\", \"PPP conversion factor, GDP (LCU per international $)\",\"GDP deflator: linked series (base year varies by country)\"]\n",
    "    gdpTable = WDI_Data.loc[(WDI_Data['Country_Name'] == country) & WDI_Data['Indicator_Name'].isin(gdparray)]\n",
    "    newValue = (value*1000000000) * gdpTable.loc[(gdpTable['Indicator_Name'] == \"PPP conversion factor, GDP (LCU per international $)\")][\"2005\"].iloc[0]\n",
    "    newValue = ((newValue)/gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (current LCU)\")][\"2005\"].iloc[0])*(gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (constant LCU)\")][\"2005\"].iloc[0])\n",
    "    newValue = (newValue/gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (constant LCU)\")][\"2010\"].iloc[0]) * gdpTable.loc[(gdpTable['Indicator_Name'] == \"GDP (current US$)\")][\"2010\"].iloc[0]\n",
    "    \n",
    "    del gdpTable, gdparray\n",
    "    \n",
    "    return newValue\n",
    "\n",
    "def industryProjector(country, year, income, thisSSP):\n",
    "    industryData = WDI_Data.loc[(WDI_Data['Country_Name'] == country) & (WDI_Data['Indicator_Name'] == \"Industry (including construction), value added (% of GDP)\")]\n",
    "    industryData=industryData.drop(industryData.columns[0:34],axis=1)\n",
    "    industryData=industryData.drop(['Unnamed:_64'],1)\n",
    "    industryData=industryData.drop(industryData.columns[25:],axis=1)\n",
    "    industryData = industryData.to_numpy().reshape(-1)\n",
    "    x = arange(1990,2015)\n",
    "    indPrediction = interp(year, x, industryData)\n",
    "    rate = predictions[(predictions['INDICATOR'] == \"IND\")& (predictions['TYPE'] == income)][thisSSP].iloc[0]\n",
    "    newRate = ( ( (rate/40)*( 40-(2050-int(year)) ) )/100)+1\n",
    "    \n",
    "    del industryData, rate\n",
    "    \n",
    "    return indPrediction * newRate\n",
    "\n",
    "def createInputData(country, year, thisSSP):\n",
    "    code = countryConverter(country)\n",
    "    income = countryIncome(country)\n",
    "    OECD = countryOECD(country)\n",
    "    \n",
    "    latest, CO2 = getYearData(country, \"2010\")\n",
    "    \n",
    "    thisCountry = SSP_Data.loc[SSP_Data['REGION'] == code]\n",
    "    \n",
    "    population = thisCountry[(thisCountry['VARIABLE'] == \"Population\") & (thisCountry['MODEL'] == \"OECD Env-Growth\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]*1000000\n",
    "    urbanisation = thisCountry[(thisCountry['VARIABLE'] == \"Population|Urban|Share\") & (thisCountry['MODEL'] == \"NCAR\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]\n",
    "    trade = interpolater(2010, 2050, latest[0][6], predictions[(predictions['INDICATOR'] == \"TRD\")][thisSSP].iloc[0] ,int(year))\n",
    "    fdi = interpolater(2010, 2050, latest[0][1], predictions[(predictions['INDICATOR'] == \"FDI\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0] ,int(year))\n",
    "    rnd = latest[0][4]*(((predictions[(predictions['INDICATOR'] == \"RND\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)+1)**(int(year)-2010))\n",
    "    #enr = latest[0][0]*(((predictions[(predictions['INDICATOR'] == \"ENR\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)+1)**(int(year)-2010))\n",
    "    enr = latest[0][0]+((latest[0][0]*((predictions[(predictions['INDICATOR'] == \"ENR\")& (predictions['TYPE'] == OECD)][thisSSP].iloc[0]/100)))*(int(year)-2010))\n",
    "    gdp = (getGDPInfo(country, thisCountry[(thisCountry['VARIABLE'] == \"GDP|PPP\") & (thisCountry['MODEL'] == \"OECD Env-Growth\") & (thisCountry['SSP'] == thisSSP)][year].iloc[0]))/population\n",
    "    ind = industryProjector(country, year, income, thisSSP)\n",
    "    rate = predictions[(predictions['INDICATOR'] == \"HDI\")& (predictions['TYPE'] == income)][thisSSP].iloc[0]\n",
    "    hdio = getHDI(hdi_data, country)\n",
    "    hdi = HDIProjector(hdio.iloc[[20]].iloc[0].iloc[0], year, rate)\n",
    "    pv =getPower(pv_data, country)\n",
    "    raw_hyd=getPower(hyd_data, country)                   \n",
    "    area = getArea(country)\n",
    "    hyd = raw_hyd/area\n",
    "    \n",
    "    den = population/area\n",
    "    \n",
    "    thisInputData = [[enr, fdi, gdp, ind, rnd, den, trade, urbanisation, pv, hyd.iloc[0],  hdi]]\n",
    "    \n",
    "    del code, den, income, OECD, latest, CO2, thisCountry, population, urbanisation, trade, fdi, rnd, enr, gdp, ind, rate, hdio, hdi, pv, hyd\n",
    "    \n",
    "    return thisInputData\n",
    "\n",
    "def SSP_Projection(country, yearRange):\n",
    "    if not os.path.exists('Projections/Grouped/{}/'.format(country)):\n",
    "        os.makedirs('Projections/Grouped/{}/'.format(country))\n",
    "    \n",
    "    newNP = np.zeros((1,1))\n",
    "    countrySummary = pd.DataFrame(data=newNP[1:,1:])\n",
    "    \n",
    "    for SSP in SSPS:\n",
    "        start = timer()\n",
    "\n",
    "        newNP = np.zeros((1,1))\n",
    "        SSPResults = pd.DataFrame(data=newNP[1:,1:])\n",
    "        \n",
    "        for year in yearRange:\n",
    "            \n",
    "            thisResults = predictAll(createInputData(country, year, SSP))\n",
    "            \n",
    "            SSPResults = pd.concat([SSPResults, thisResults], axis=1)\n",
    "\n",
    "            del thisResults\n",
    "            \n",
    "            \n",
    "        SSPResults.columns = yearRange\n",
    "        characteristics = SSPResults.describe()\n",
    "\n",
    "        SSPStd = characteristics.iloc[2]\n",
    "        ConfInt = (SSPStd/10)*1.96\n",
    "        lowerInt = characteristics.iloc[1] - ConfInt\n",
    "        upperInt = characteristics.iloc[1] + ConfInt\n",
    "        \n",
    "        plotting = pd.concat([lowerInt,characteristics.iloc[1],upperInt], axis=1)\n",
    "        plotting.columns = [SSP+\" lower\", SSP+\" mean\", SSP+\" upper\"]\n",
    "        convertedPlotting = plotting.T\n",
    "        countrySummary = pd.concat([countrySummary, convertedPlotting])\n",
    "        \n",
    "        SSPResults = pd.concat([SSPResults, characteristics])\n",
    "        SSPResults.to_csv('Projections/Grouped/{}/{}.csv'.format(country, SSP))\n",
    "        \n",
    "        end = timer()\n",
    "        secondsDuration = (end - start)\n",
    "        duration = time.strftime('%H:%M:%S', time.gmtime(secondsDuration))\n",
    "        print(country + \" \" + SSP +  \": \" + duration)\n",
    "\n",
    "        del start, end, duration, convertedPlotting, plotting, newNP, SSPResults, SSPStd, ConfInt, lowerInt, upperInt, characteristics\n",
    "        \n",
    "    countrySummary.to_csv('Projections/Grouped/{}/Summary.csv'.format(country))\n",
    "    \n",
    "    del countrySummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:21:04: 1. China\n",
      "China SSP1: 00:00:55\n",
      "China SSP2: 00:00:53\n",
      "China SSP3: 00:00:53\n",
      "China SSP4: 00:00:53\n",
      "China SSP5: 00:00:53\n",
      "\n",
      "\n",
      "17:25:34: 2. India\n",
      "India SSP1: 00:00:54\n",
      "India SSP2: 00:00:53\n",
      "India SSP3: 00:00:52\n",
      "India SSP4: 00:00:52\n",
      "India SSP5: 00:00:52\n",
      "\n",
      "\n",
      "17:30:00: 3. Russian Federation\n",
      "Russian Federation SSP1: 00:00:53\n",
      "Russian Federation SSP2: 00:00:53\n",
      "Russian Federation SSP3: 00:00:53\n",
      "Russian Federation SSP4: 00:00:54\n",
      "Russian Federation SSP5: 00:00:53\n",
      "\n",
      "\n",
      "17:34:28: 4. Brazil\n",
      "Brazil SSP1: 00:00:54\n",
      "Brazil SSP2: 00:00:54\n",
      "Brazil SSP3: 00:00:56\n",
      "Brazil SSP4: 00:00:53\n",
      "Brazil SSP5: 00:00:53\n",
      "\n",
      "\n",
      "17:39:01: 5. Saudi Arabia\n",
      "Saudi Arabia SSP1: 00:00:53\n",
      "Saudi Arabia SSP2: 00:00:53\n",
      "Saudi Arabia SSP3: 00:00:53\n",
      "Saudi Arabia SSP4: 00:00:53\n",
      "Saudi Arabia SSP5: 00:00:53\n",
      "\n",
      "\n",
      "17:43:28: 6. Pakistan\n",
      "Pakistan SSP1: 00:00:53\n",
      "Pakistan SSP2: 00:00:53\n",
      "Pakistan SSP3: 00:00:53\n",
      "Pakistan SSP4: 00:00:53\n",
      "Pakistan SSP5: 00:00:53\n",
      "\n",
      "\n",
      "17:47:56: 7. Mexico\n",
      "Mexico SSP1: 00:00:53\n",
      "Mexico SSP2: 00:00:54\n",
      "Mexico SSP3: 00:00:52\n",
      "Mexico SSP4: 00:00:51\n",
      "Mexico SSP5: 00:00:51\n",
      "\n",
      "\n",
      "17:52:19: 8. Iran, Islamic Rep.\n",
      "Iran, Islamic Rep. SSP1: 00:00:51\n",
      "Iran, Islamic Rep. SSP2: 00:00:51\n",
      "Iran, Islamic Rep. SSP3: 00:00:52\n",
      "Iran, Islamic Rep. SSP4: 00:00:51\n",
      "Iran, Islamic Rep. SSP5: 00:00:51\n",
      "\n",
      "\n",
      "17:56:38: 9. Turkey\n",
      "Turkey SSP1: 00:00:51\n",
      "Turkey SSP2: 00:00:51\n",
      "Turkey SSP3: 00:00:51\n",
      "Turkey SSP4: 00:00:51\n",
      "Turkey SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:00:56: 10. Indonesia\n",
      "Indonesia SSP1: 00:00:50\n",
      "Indonesia SSP2: 00:00:51\n",
      "Indonesia SSP3: 00:00:51\n",
      "Indonesia SSP4: 00:00:51\n",
      "Indonesia SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:05:13: 11. South Africa\n",
      "South Africa SSP1: 00:00:51\n",
      "South Africa SSP2: 00:00:51\n",
      "South Africa SSP3: 00:00:51\n",
      "South Africa SSP4: 00:00:51\n",
      "South Africa SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:09:32: 12. Vietnam\n",
      "Vietnam SSP1: 00:00:51\n",
      "Vietnam SSP2: 00:00:51\n",
      "Vietnam SSP3: 00:00:51\n",
      "Vietnam SSP4: 00:00:51\n",
      "Vietnam SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:13:49: 13. Egypt, Arab Rep.\n",
      "Egypt, Arab Rep. SSP1: 00:00:51\n",
      "Egypt, Arab Rep. SSP2: 00:00:51\n",
      "Egypt, Arab Rep. SSP3: 00:00:51\n",
      "Egypt, Arab Rep. SSP4: 00:00:51\n",
      "Egypt, Arab Rep. SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:18:08: 14. Thailand\n",
      "Thailand SSP1: 00:00:51\n",
      "Thailand SSP2: 00:00:52\n",
      "Thailand SSP3: 00:00:51\n",
      "Thailand SSP4: 00:00:51\n",
      "Thailand SSP5: 00:00:52\n",
      "\n",
      "\n",
      "18:22:28: 15. Argentina\n",
      "Argentina SSP1: 00:00:52\n",
      "Argentina SSP2: 00:00:52\n",
      "Argentina SSP3: 00:00:51\n",
      "Argentina SSP4: 00:00:51\n",
      "Argentina SSP5: 00:00:51\n",
      "\n",
      "\n",
      "18:26:48: 16. Nigeria\n",
      "Nigeria SSP1: 00:00:51\n",
      "Nigeria SSP2: 00:00:51\n",
      "Nigeria SSP3: 00:00:53\n",
      "Nigeria SSP4: 00:00:51\n",
      "Nigeria SSP5: 00:00:53\n",
      "\n",
      "\n",
      "18:31:10: 17. Poland\n",
      "Poland SSP1: 00:00:51\n",
      "Poland SSP2: 00:00:52\n",
      "Poland SSP3: 00:00:52\n",
      "Poland SSP4: 00:00:52\n",
      "Poland SSP5: 00:00:52\n",
      "\n",
      "\n",
      "18:35:32: 18. Malaysia\n",
      "Malaysia SSP1: 00:00:51\n",
      "Malaysia SSP2: 00:00:51\n",
      "Malaysia SSP3: 00:00:53\n",
      "Malaysia SSP4: 00:00:54\n",
      "Malaysia SSP5: 00:00:53\n",
      "\n",
      "\n",
      "18:39:58: 19. Venezuela, RB\n",
      "Venezuela, RB SSP1: 00:00:53\n",
      "Venezuela, RB SSP2: 00:00:53\n",
      "Venezuela, RB SSP3: 00:00:53\n",
      "Venezuela, RB SSP4: 00:00:53\n",
      "Venezuela, RB SSP5: 00:00:55\n",
      "\n",
      "\n",
      "18:44:26: 20. Congo, Dem. Rep.\n",
      "Congo, Dem. Rep. SSP1: 00:00:54\n",
      "Congo, Dem. Rep. SSP2: 00:00:54\n",
      "Congo, Dem. Rep. SSP3: 00:00:53\n",
      "Congo, Dem. Rep. SSP4: 00:00:53\n",
      "Congo, Dem. Rep. SSP5: 00:00:53\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, country in enumerate(IMFEmerging):\n",
    "    startStamp = str(datetime.now().strftime('%H:%M:%S'))\n",
    "    print(startStamp + \": \" + str(x+1) + \". \" + country)\n",
    "    SSP_Projection(country, allYears)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK2019 = [[2630.53, 0.0790593814371999, 42328.9002575769, 17.4206873962023, 105674, 66836327, 64.2877735623301, 83.652]]\n",
    "UK2018 = [[2757.69, 2.83706174602294, 43043.22782, 17.5188312851163, 94953, 66460344, 62.6190596109507, 83.398]]\n",
    "UK2015 = [[2764.516671, 1.547962, 44974.831877, 18.141621, 57891.0, 65116219.0, 56.683096, 82.626]]\n",
    "USA2017 = [[7547.57012, 1.820076, 59957.725851, 18.20794, 448211.0, 324985539.0, 27.14232, 82.058]]\n",
    "\n",
    "predictionSet = predictAll(\"United Kingdom\", UK2018)\n",
    "predictionSet.to_csv('Projections/History_Check/Adjusted_Input/UK2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = [\"China\",\n",
    "\"India\",\n",
    "\"Russian Federation\",\n",
    "\"Brazil\",\n",
    "\"Saudi Arabia\",\n",
    "\"Pakistan\",\n",
    "\"Mexico\"]\n",
    "\n",
    "batch2 = [\n",
    "    \"Iran, Islamic Rep.\",\n",
    "\"Turkey\",\n",
    "\"Indonesia\",\n",
    "\"South Africa\",\n",
    "\"Vietnam\",\n",
    "\"Egypt, Arab Rep.\",\n",
    "\"Thailand\"\n",
    "]\n",
    "\n",
    "batch3 = [\n",
    "    \"Argentina\",\n",
    "\"Nigeria\",\n",
    "\"Poland\",\n",
    "\"Malaysia\",\n",
    "\"Venezuela, RB\",\n",
    "\"Congo, Dem. Rep.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
